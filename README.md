# LLM Symbolic Planning without Experts Project
![Pipeline Overview](./pipeline_overview.png)

This repository contains the codebase for the paper **Planning in the Dark: LLM-Symbolic Planning Pipeline without Experts #1843** 

> ***TL;DR:*** We introduce a novel LLM-symbolic planning pipeline that eliminates the dependency on expert intervention by automatically generating, validating, and ranking action schemas. This paves the way for more scalable and accessible planning systems.

>[!IMPORTANT]
> This project utilizes the Kedro framework to enhance code modularity and ensure **reproducibility** to the greatest extent possible. If you encounter any issues related to **reproducibility**, please report them to the authors.


## Directory Structure
```bash
ðŸ“‚ Official-LLM-Symbolic-Planning-without-Experts
â”œâ”€â”€ ðŸ“˜ README.md
â”œâ”€â”€ ðŸ“‚ conf
â”‚   â”œâ”€â”€ ðŸ“‚ base
â”‚   â””â”€â”€ ðŸ“‚ local
â”‚       â””â”€â”€ ðŸ” credentials.yml   # Add your LLM api key here
â”œâ”€â”€ ðŸ“‚ data
â”‚   â”œâ”€â”€ ðŸ“‚ 01_raw
â”‚   â”‚   â”œâ”€â”€ ðŸ“˜ Co-Star-Prompting.md
â”‚   â”‚   â””â”€â”€ ðŸ“‚ pddl_domain # the planning tasks for training, validation and testing, including CoT prompt templates 
â”‚   â”œâ”€â”€ ðŸ“‚ 02_intermediate
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ action_schema_combination # the output of the action schema combination pipeline
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ pddl_domain # training data for finetuning the sentence encoder
â”‚   â”‚   â””â”€â”€ ðŸ“‚ post_generate_schema_pool # action schema pool after syntax correction
â”‚   â”œâ”€â”€ ðŸ“‚ 03_primary
â”‚   â”‚   â””â”€â”€ ðŸ“‚ cp_threshold # conformal prediction threshold information
â”‚   â”œâ”€â”€ ðŸ“‚ 06_models # sentence encoder models checkpoint
â”‚   â”‚   â””â”€â”€ ðŸ“‚ finetuned_sentence_encoder_batch_256_2024-07-06_20-18-22
â”‚   â”œâ”€â”€ ðŸ“‚ 07_model_output
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ llm_to_domain_to_plans    # generated plans from the LLM
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ pure_generate_schema_pool # conversation details of LLM-symbolic pipeline
â”‚   â”‚   â””â”€â”€ ðŸ“‚ tree_of_thought_plans    # conversation details of ToT approach
â”‚   â””â”€â”€ ðŸ“‚ 08_reporting
â”‚       â”œâ”€â”€ ðŸ“‚ action_combi_analysis # action schema combination analysis
â”‚       â”œâ”€â”€ ðŸ“‚ cosine_sim_comparison_after_finetune  # cosine sim comparison after finetuning
â”‚       â”œâ”€â”€ ðŸ“‚ cosine_sim_comparison_of_vanilla_model # cosine sim comparison of vanilla model
â”‚       â””â”€â”€ ðŸ“‚ human_evaluation_results  # human evaluation results
â”œâ”€â”€ ðŸ“‚ job-scripts       # All the reference job scripts to run the pipeline
â”‚   â”œâ”€â”€ ðŸ’» job_acquire_plan_and_ranking.sh
â”‚   â”œâ”€â”€ ðŸ’» job_action_combination_and_analysis_cp_false.sh
â”‚   â”œâ”€â”€ ðŸ’» job_action_combination_and_analysis_cp_true.sh
â”‚   â”œâ”€â”€ ðŸ’» job_analyzing_cos_sim_score_for_vanilla_model.sh
â”‚   â”œâ”€â”€ ðŸ’» job_conformal_prediction_threshold_cal.sh
â”‚   â”œâ”€â”€ ðŸ’» job_finetune_sentence_encoder.sh
â”‚   â”œâ”€â”€ ðŸ’» job_finetune_sentence_encoder_smaller.sh
â”‚   â”œâ”€â”€ ðŸ’» job_generate_schema_pool.sh
â”‚   â”œâ”€â”€ ðŸ’» job_post_finetune_analysis.sh
â”‚   â”œâ”€â”€ ðŸ’» job_post_generate_schema_pool_parsing.sh
â”‚   â”œâ”€â”€ ðŸ’» job_setup_sentence_encoder.sh
â”‚   â”œâ”€â”€ ðŸ’» job_tot_planning.sh
â”‚   â””â”€â”€ ðŸ’» job_tot_planning_test_gpt_on_sussman_anomaly.sh
â”œâ”€â”€ ðŸ“‚ opt           # external tool (classical planner) to generate plans
â”‚   â”œâ”€â”€ ðŸ“˜ README.md
â”‚   â””â”€â”€ ðŸ“‚ planning-as-a-service
â”‚       â””â”€â”€ ðŸ“˜ README.md
â”œâ”€â”€ ðŸ“œ pyproject.toml
â”œâ”€â”€ ðŸ“œ requirements.txt
â””â”€â”€ ðŸ“‚ src
    â””â”€â”€ ðŸ“‚ better_leveraging_llm_to_construct_world_models
       â””â”€â”€ ðŸ“‚ pipelines         # All modules of the LLM-symbolic planning pipline
          â”œâ”€â”€ ðŸ“‚ action_schema_combination # get all schema set combinations and obtaining viable action schema sets by using classical planner
          â”œâ”€â”€ ðŸ“‚ bisim_evaluation # deprecated because it require the parameters of actions to be the same as the reference model
          â”œâ”€â”€ ðŸ“‚ compare_cos_sim_between_act_and_nl_desc # calculate the cos. sim. between the action schema and the natural language description
          â”œâ”€â”€ ðŸ“‚ conformal_prediction_filtering    # calculate the conformal prediction threshold based on the validation dataset.
          â”œâ”€â”€ ðŸ“‚ finetuning_sentence_encoder   # finetune the sentence encoder
          â”œâ”€â”€ ðŸ“‚ generate_schema_pool                 # generate action schema pool
          â”œâ”€â”€ ðŸ“‚ plan_evaluation   # human blind evaluation on the plan quality
          â”œâ”€â”€ ðŸ“‚ post_construction_ranking # obtaining ranking score for the plan candidates
          â”œâ”€â”€ ðŸ“‚ post_generate_schema_pool # Validate the syntax of the generated domain models
          â”œâ”€â”€ ðŸ“‚ setup_sentence_encoder    # init the sentence encoder
          â””â”€â”€ ðŸ“‚ tree_of_thought_direct_planning # ToT approach to generate plans

```

>[!TIP]
> There are readme files in the subdirectories of the project. Please refer to them for more detailed information.


## Installation Steps
1. Setup the environment
```bash
conda create --name=btrpddl python=3.10
conda activate btrpddl
# change the directory to the project root
pip install -e .
# go to opt/planning-as-a-service and follow the README.md to setup the classical planner
```

2. Provide API key for the LLMs models
   - please add your API key in `conf/local/credentials.yml`
## How to Use
To run the pipeline, follow the following steps:

1. Checking and updating the parameters in the `conf` directory
2. `kedro run -p` the pipeline. A suggested order is provided in `.vscode/launch.json` for reference. We suggest to check job scripts in `job-scripts` for the reference of running the pipeline.


```json
"configurations": [
        {
            "name": "Python: Kedro Run Cross Encoder Setup",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "module": "kedro",
            "args": ["run", "-p", "setup_sentence_encoder"]
            // Any other arguments should be passed as a comma-seperated-list
            // e.g "args": ["run", "--pipeline", "pipeline_name"]
        },
        {
            "name": "Python: Kedro Run Analyze Cosine Sim",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "module": "kedro",
            "args": ["run", "--from-nodes=init_cross_encoder_model_node", "--to-nodes=generate_boxplot_from_cos_sim_data_node",
            "--params", "setup_sentence_encoder_cfg.model_type=bi_encoder"]
        },
        {
            "name": "Python: Kedro Run FineTune Sentence Encoder",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "module": "kedro",
            "args": ["run", "-p", "finetuning_sentence_encoder"]
        },

        {
            "name": "Python: Kedro Run FineTune Google T5",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "module": "kedro",
            "args": ["run", "-p", "finetuning_sentence_encoder", "--params", "finetuning_encoder_cfg.train_batch_size=16,setup_sentence_encoder_cfg.model_name=google/flan-t5-xl,setup_sentence_encoder_cfg.model_type=bi_encoder,finetuning_encoder_cfg.is_finetune_complete=false,finetuning_encoder_cfg.train_batch_size=1024"]
        },
        {
            "name": "Python: Kedro Run LLM Query Ensemble",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "module": "kedro",
            "args": ["run", "-p", "generate_schema_pool"]
        },
        {
            "name": "Python: Kedro Run ToT Planning",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "module": "kedro",
            "args": ["run", "-p", "tree_of_thought_direct_planning", "--params", "tot_direct_planning_cfg.tree_breadth=4"]
        },
        {
            "name": "Python: Kedro Run Post Majority Vote Parsing",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "module": "kedro",
            "args": ["run", "-p", "post_generate_schema_pool"]
        },
        {
            "name": "Python: Kedro Run CP calculation",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "module": "kedro",
            "args": ["run", "-p", "conformal_prediction_filtering"]
        },
        {
            "name": "Python: Kedro action combination",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "module": "kedro",
            "args": ["run", "-p", "action_schema_combination", "--params", "use_cp=false"]
        },
        {
            "name": "Python: Kedro Post Construction Ranking",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "module": "kedro",
            "args": ["run", "-p", "post_construction_ranking"]
        },
        {
            "name": "Python: Kedro Human Evaluation",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "module": "kedro",
            "args": ["run", "-p", "plan_evaluation"]
        },
    ]
```

>[!NOTE]
> The experiments require you to provide the API token for the LLMs models. If you encounter any issues, please submit an issue in the repository. Common issues are: 1) some models uses the absolute path, requiring the user to modify them manually, 2) the API token is not provided in the `conf/local/credentials.yml` file. 3) tools in the opt folder are not installed properly.